{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This code will develop a Retrieval Augemented Generation (RAG) based chat bot using LangChain, PineCone and OpenAI with the following salient points\n",
    "- Read the JSON question/answer pairs of a company\n",
    "- Convert each question/answer pair into a separate text chunk using LangChain TextSplitter\n",
    "- Convert the chunked question/answer pair into embeddings using OpenAI Embeddings\n",
    "- Create an Index in Pinecone and upsert the embedded chunks in the Pinecone vector\n",
    "- Take the query from user, vectorize it and then find the matching text chunk from Pinecone vector store\n",
    "- Include the matching text chunk into the query which is to be sent to LLM to answer the specific question about the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting the Vector Database into the PineCone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reading the API KEYS for OpenAI and Pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "creds_file = \"../credentials.json\"\n",
    "    \n",
    "with open(creds_file, 'r') as file:\n",
    "    creds_data = json.load(file)\n",
    "    openai_api_key = creds_data['OPENAI_API_KEY']\n",
    "    pinecone_api_key = creds_data['PINECONE_API_KEY']\n",
    "\n",
    "assert openai_api_key != None, \"\"\n",
    "assert pinecone_api_key != None, \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading the data from the text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_loader = TextLoader(file_path='scalexi.txt', encoding=\"utf-8\")\n",
    "documents = file_loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dividing the complete text into overlapping chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be multiple ways of chunking the texts. We can do that by a specific heading or just randomly chunk that as it is done in the tutorial. It varies application to application and really depends on what context the model might need to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "chunked_text = [doc.page_content for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Defining the embeddings model to create the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model = OpenAIEmbeddings(api_key=openai_api_key, model=\"text-embedding-ada-002\")\n",
    "chunked_embeddings = embed_model.embed_documents(chunked_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the embeddings and Chunked Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_short_id(content: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a short ID based on the content using SHA-256 hash.\n",
    "\n",
    "    Args:\n",
    "    - content (str): The content for which the ID is generated.\n",
    "\n",
    "    Returns:\n",
    "    - short_id (str): The generated short ID.\n",
    "    \"\"\"\n",
    "    hash_obj = hashlib.sha256()\n",
    "    hash_obj.update(content.encode(\"utf-8\"))\n",
    "    return hash_obj.hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_metadata = []\n",
    "\n",
    "for doc_text, embedding in zip(chunked_text, chunked_embeddings):\n",
    "    doc_id = generate_short_id(doc_text)\n",
    "    \n",
    "    data_item = {\n",
    "        \"id\": doc_id,\n",
    "        \"values\": embedding,\n",
    "        \"metadata\": {'text':doc_text} #include the text as metadata\n",
    "    }\n",
    "    \n",
    "    data_with_metadata.append(data_item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the PineCone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'indexes': [{'deletion_protection': 'disabled',\n",
       "              'dimension': 1536,\n",
       "              'host': 'rag-chatbot-description-ott2zv7.svc.aped-4627-b74a.pinecone.io',\n",
       "              'metric': 'dotproduct',\n",
       "              'name': 'rag-chatbot-description',\n",
       "              'spec': {'serverless': {'cloud': 'aws', 'region': 'us-east-1'}},\n",
       "              'status': {'ready': True, 'state': 'Ready'}}]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# configure client\n",
    "pc = Pinecone(api_key=pinecone_api_key)\n",
    "\n",
    "# configure serverless spec\n",
    "spec = ServerlessSpec(cloud='aws', region='us-east-1')\n",
    "\n",
    "pc.list_indexes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for and delete index if already exists\n",
    "index_name = 'rag-chatbot-description'\n",
    "if index_name in pc.list_indexes().names():\n",
    "    pc.delete_index(index_name)\n",
    "\n",
    "# we create a new index\n",
    "pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "\n",
    "# Wait until the index is ready\n",
    "while not pc.describe_index(index_name).status['ready']:\n",
    "    time.sleep(1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Connecting to Index and Upserting our knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upserting our dataset into the Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'upserted_count': 6}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.upsert(vectors=data_with_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 1536,\n",
       " 'index_fullness': 0.0,\n",
       " 'namespaces': {},\n",
       " 'total_vector_count': 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering queries using information present in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_field = \"text\" # it represents the field with which the source text is present in the pinecone index\n",
    "vectorstore = PineconeVectorStore(index, embed_model, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"Tell me something about Scalex?\"\n",
    "# vectorstore.similarity_search(query, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=openai_api_key,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To only answer the question to each query directly without remembring the past information, use `RetrievalQA.from_chain_type`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleX Innovation is a pioneering leader in the realm of Generative AI and Large Language Models. They specialize in integrating transformative technologies into business strategies to enhance innovation and operational efficiency. ScaleX Innovation offers tailored solutions such as automating workflows, content analysis, and custom model implementations across multiple industry verticals. They are dedicated to ethical compliance and versatility, making them a trusted partner for businesses worldwide.\n"
     ]
    }
   ],
   "source": [
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "query = \"Tell me something about Scalex?\"\n",
    "print(qa.run(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remember the information about the past conversation between the user and the robot, implement `ConversationalRetrievalChain.from_llm` with `chat_history` as the memory buffer. Please remember to use this if it is required since this will lead to more usage of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/mn27889/miniconda3/envs/rag-chatbot/lib/python3.11/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleX Innovation is a pioneering leader in the realm of Generative AI and Large Language Models. They specialize in integrating these transformative technologies into business strategies to enhance innovation and operational efficiency. ScaleX Innovation offers tailored solutions for automating workflows, content analysis, and custom model implementations. They are committed to bridging the gap between technology and business, with a focus on ethical compliance and versatility. Their expertise extends across multiple industry verticals, making them a trusted partner for businesses worldwide.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you tell me about ScaleX Innovation?\"\n",
    "answer = conversation_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ScaleX Innovation is a pioneering leader in the realm of Generative AI and Large Language Models. They specialize in offering bespoke solutions that drive innovation, automate workflows, and enable unprecedented efficiencies for businesses. Their expertise extends across multiple industry verticals, ensuring that businesses can harness the power of AI-driven digital transformation. ScaleX Innovation is committed to bridging the gap between technology and business, offering specialized services like cross-domain consultation, business automation, and a client-centric approach. They are known for their adaptive AI solutions that can scale and adapt to diverse industrial requirements and challenges.\n"
     ]
    }
   ],
   "source": [
    "query = \"Please tell me more\"\n",
    "answer = conversation_chain.run(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chatbot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
